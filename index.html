<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"

   Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
  <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      Towards Smarter Segmentation: Improving SAM's Anomaly Understanding with
      RLHF
    </title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Towards Smarter Segmentation: Improving SAM's Anomaly
                Understanding with RLHF
              </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                  <a href="https://github.com/slmowan" target="_blank"
                    >Wan Wang</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://github.com/novia-aa" target="_blank"
                    >Yu-Tong Chuang</a
                  ><sup>*</sup>,
                </span>
                  <span class="author-block">
                  <a href="https://github.com/Charlesczc" target="_blank"
                    >Yiu Chang</a
                  ><sup>*</sup>,
                  </span>
                <span class="author-block">
                  <a href="https://github.com/lulinliu" target="_blank"
                    >Lulin Liu</a
                  ><sup>*</sup>,</span
                >
                  </div>

                  <div class="is-size-5 publication-authors">
                <span class="author-block"
                  >University of Minnesota, Twin Cities<br
                /></span>
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Indicates Equal Contribution</small
                  ></span
                >
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                    <a
                      href="static/pdfs/5541_poster_final_version.pptx.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/slmowan/sam-finetune.git"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

    <!-- 
Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
End teaser video 
-->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
                Industrial anomaly detection relies heavily on precise
                segmentation, yet quality labeled data is scarce due to high
                annotation costs. The Segment Anything Model (SAM), despite
                robust zero-shot capabilities, struggles to effectively detect
                anomalies. This project introduces Reinforcement Learning with
                Human Feedback (RLHF) to improve SAM's anomaly segmentation. By
                incorporating expert human feedback into training via a reward
                model and reinforcement learning (PPO), our method significantly
                enhances SAM's accuracy for anomaly detection. This work
                pioneers human-feedback-driven fine-tuning for vision foundation
                models, aiming for practical, efficient anomaly segmentation in
                industrial applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

    <!-- Project Progress Section -->
    <!-- <section class="section"> -->
      <!-- <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Project Progress</h2>

            Phase 1
            <h3 class="title is-4">
              Phase 1: Benchmarking SAM's Anomaly Segmentation
            </h3>
            <div class="content has-text-justified">
              <p>
                We have successfully completed the first phase of our project,
                focusing on establishing SAM's baseline performance on anomaly
                segmentation tasks. Our evaluation revealed several key
                insights:
              </p>

              <ul>
                <li>
                  <strong>Dataset Preparation:</strong> We processed multiple
                  industrial anomaly detection datasets, standardizing formats
                  and annotations to ensure consistent evaluation.
                </li>

                <li>
                  <strong>Performance Analysis:</strong> While SAM excels at
                  general segmentation tasks, we identified systematic
                  limitations when handling industrial anomalies, particularly
                  subtle surface defects and texture irregularities.
                </li>

                <li>
                  <strong>Failure Case Categorization:</strong> We categorized
                  SAM's failure patterns into three main types: missed
                  detections (false negatives), imprecise boundary delineation,
                  and over-segmentation of normal regions.
                </li>

                <li>
                  <strong>Confidence Analysis:</strong> We measured SAM's
                  prediction confidence across different types of anomalies,
                  revealing that SAM often exhibits high confidence even for
                  incorrect segmentations, highlighting the need for
                  domain-specific adaptation.
                </li>
              </ul>

              <p>
                These findings establish a critical baseline for measuring
                improvement in subsequent phases and confirm our hypothesis that
                while powerful, foundation models like SAM require specialized
                fine-tuning to excel at domain-specific tasks like industrial
                anomaly detection.
              </p>

              <p>
                Moving forward, we are now working on Phase 2, developing a
                domain-specific baseline model inspired by MedSAM's approach to
                medical image segmentation, adapting these techniques to the
                industrial anomaly context.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Project Progress Section -->

    <!-- Image carousel section with summaries -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
              <img
                src="static/images/carousel1.jpg"
                alt="SAM Anomaly Segmentation - Bottle"
              />
        <h2 class="subtitle has-text-centered">
                <strong>Good Segmentation, High Confidence - Bottle</strong
                ><br />
                SAM correctly identifies and segments the anomaly with high
                confidence, showing its potential when anomalies have clear
                boundaries and distinct features from the background.
        </h2>
      </div>
      <div class="item">
              <img
                src="static/images/carousel2.jpg"
                alt="SAM Anomaly Segmentation - Capsule"
              />
        <h2 class="subtitle has-text-centered">
                <strong>Poor Segmentation, High Confidence - Carpet</strong
                ><br />
                SAM incorrectly segments the anomaly while expressing high
                confidence, demonstrating how misleading confidence scores can
                be for textured surfaces where anomalies have subtle differences
                from the background pattern.
        </h2>
      </div>
      <div class="item">
              <img
                src="static/images/carousel3.jpg"
                alt="SAM Anomaly Segmentation - Hazelnut"
              />
        <h2 class="subtitle has-text-centered">
                <strong>Good Segmentation, Low Confidence - Hazelnut</strong
                ><br />
                SAM accurately detects the anomaly but with low confidence,
                showing how even correct predictions can be uncertain,
                suggesting potential for performance improvement through
                confidence calibration.
       </h2>
     </div>
     <div class="item">
              <img
                src="static/images/carousel4.jpg"
                alt="SAM Anomaly Segmentation - Plate"
              />
      <h2 class="subtitle has-text-centered">
                <strong>Poor Segmentation, Low Confidence - Plate</strong><br />
                SAM fails to detect the anomaly and expresses low confidence,
                highlighting the most challenging case where both segmentation
                and confidence need improvement through specialized training.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->

    <!-- Phase 2 Section -->
    <!-- <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h3 class="title is-4">
              Phase 2: Domain-Specific Model Development
            </h3>
            <div class="content has-text-justified">
              <p>
                We have made significant progress in Phase 2, focusing on data
                preprocessing and adapting SAM for industrial anomaly detection.
                Our work has successfully addressed several technical
                challenges:
              </p>

              <h4 class="title is-5">Data Preprocessing Innovations</h4>
              <ul>
                <li>
                  <strong>Input Resolution Standardization:</strong> SAM
                  requires 1024×1024 resolution inputs, but industrial images
                  come in various dimensions. We implemented a robust
                  preprocessing pipeline that preserves aspect ratios while
                  meeting SAM's requirements:
                </li>
                <ul>
                  <li>
                    For non-square images, we apply careful padding to both
                    images and their corresponding ground truth masks, ensuring
                    spatial relationships are maintained
                  </li>
                  <li>
                    This approach preserves the relative size and location of
                    anomalies, which is critical for accurate segmentation
                  </li>
                </ul>

                <li>
                  <strong>Multiple Anomaly Handling:</strong> Industrial images
                  often contain multiple anomalies with separate bounding boxes.
                  We developed a sophisticated approach to handle these cases:
                </li>
                <ul>
                  <li>
                    For images with multiple anomalies, we create individual
                    copies of the image-mask pairs for each anomaly
                  </li>
                  <li>
                    This transformation allows the model to focus on one anomaly
                    at a time, simplifying the learning task while preserving
                    the original image context
                  </li>
                  <li>
                    During inference, predictions from multiple instances are
                    intelligently merged to provide a complete anomaly map
                  </li>
                </ul>
              </ul>

              <h4 class="title is-5">Dataset Preparation</h4>
              <p>
                Our preprocessing pipeline has been successfully applied to
                multiple industrial anomaly datasets, including MVTec AD and
                other domain-specific collections. This standardization enables:
              </p>
              <ul>
                <li>
                  Consistent training across diverse industrial products and
                  defect types
                </li>
                <li>
                  Effective utilization of SAM's existing capabilities while
                  adapting to industrial anomaly detection
                </li>
                <li>
                  Creation of a robust foundation for the upcoming RLHF training
                  phase
                </li>
              </ul>

              <p>
                As we complete Phase 2, we are preparing to initiate the RLHF
                pipeline development. The standardized dataset will serve as the
                foundation for generating multiple segmentation candidates per
                image, which will then be ranked by human annotators to train
                our reward model.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Phase 2 Section -->

    <!-- Future Work Section -->
    <!-- <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Future Work</h2>
            <div class="content has-text-justified">
              <p>
                Building on our current progress, we have outlined a comprehensive plan for the next phases of our project:
              </p>

              <h3 class="title is-5">Phase 3: RLHF-Based Training Pipeline Development</h3>
              <p>
                Our next major phase focuses on implementing the RLHF pipeline through human feedback collection and reward model training. Key deliverables include:
              </p>
              <ul>
                <li><strong>Data Pipeline Development:</strong>
                  <ul>
                    <li>Implement confidence-based mask generation system</li>
                    <li>Create automated pipeline for generating segmentation variants</li>
                    <li>Develop data processing workflows for human feedback</li>
                  </ul>
                </li>
                
                <li><strong>Human Annotation Platform:</strong>
                  <ul>
                    <li>Design and implement interface for collecting segmentation rankings</li>
                    <li>Create efficient workflow for annotators to provide feedback</li>
                    <li>Implement quality control measures for feedback collection</li>
                  </ul>
                </li>
                
                <li><strong>Reward Model Development:</strong>
                  <ul>
                    <li>Train initial reward model using collected human preferences</li>
                    <li>Implement evaluation metrics for segmentation quality</li>
                    <li>Develop feedback integration mechanisms</li>
                  </ul>
                </li>
              </ul>

              <h3 class="title is-5">Phase 4: Fine-tuning with DPO</h3>
              <p>
                The final phase will focus on implementing Direct Preference Optimization to enhance SAM's anomaly segmentation capabilities:
              </p>
              <ul>
                <li><strong>Model Enhancement:</strong>
                  <ul>
                    <li>Implement DPO-based training pipeline</li>
                    <li>Optimize model parameters using human preference data</li>
                    <li>Fine-tune segmentation performance for industrial anomalies</li>
                  </ul>
                </li>
                
                <li><strong>Performance Analysis:</strong>
                  <ul>
                    <li>Conduct comparative analysis between baseline and DPO-enhanced models</li>
                    <li>Evaluate improvements in segmentation accuracy and confidence calibration</li>
                    <li>Assess generalization to different types of industrial anomalies</li>
                  </ul>
                </li>
                
                <li><strong>Documentation and Reporting:</strong>
                  <ul>
                    <li>Prepare comprehensive documentation of methodology and results</li>
                    <li>Generate final research report with key insights</li>
                    <li>Create deployment guidelines for industrial applications</li>
                  </ul>
                </li>
              </ul>

              <p>
                Through these phases, we aim to demonstrate the effectiveness of DPO in improving SAM's anomaly detection capabilities while maintaining training stability and efficiency. Our focus will be on creating a practical, deployable solution for industrial applications.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Future Work Section -->

    <!-- Methodology Section - Updated with new pipeline image -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Methodology</h2>
            <div class="content has-text-justified">
              <p>
                We've implemented a three-stage RLHF-based segmentation fine-tuning workflow to improve SAM's anomaly detection capabilities:
              </p>
              
              <div class="columns is-centered">
                <div class="column is-full">
                  <img src="static/images/pipeline.png" alt="Three-stage RLHF Pipeline" class="center-block" style="width: 100%; margin-bottom: 1.5rem;">
                  <p class="has-text-centered is-size-6">Overview of our three-stage RLHF workflow for segmentation fine-tuning</p>
                </div>
              </div>
              
              <p>
                As shown in our pipeline diagram, our approach consists of three key stages that work together to leverage human feedback for improving model performance:
              </p>
              
              <h3 class="title is-5">Generation Stage</h3>
              <p>
                In the first stage, we combine SFT (Supervised Fine-Tuning) images with prompts to generate segmentation candidates. A fine-tuned SAM model processes these inputs through the SAM Mask Generation component, producing multiple segmentation masks for each prompt (shown as "Per-prompt Responses" in the diagram). This stage establishes a foundation of domain-specific knowledge through supervised learning.
              </p>
              
              <div class="columns is-centered">
                <div class="column is-half">
                  <img src="static/images/sam_triplet_sample.png" alt="SAM Triplet Sample" class="center-block" style="width: 100%; margin-bottom: 1.5rem;">
                  <p class="has-text-centered is-size-6">Example of a SAM-compatible triplet training sample</p>
                </div>
              </div>
              
              <h3 class="title is-5">Annotation Stage</h3>
              <p>
                The second stage introduces human judgment into the pipeline. As illustrated in the middle section of our diagram, the SFT images and corresponding mask candidates from the Generation stage are presented to human annotators who rank the segmentation options according to quality. This Human Ranking process transforms unordered segmentation candidates into preference-ranked responses, creating valuable training signal for the next stage.
              </p>
              <p>
                Our annotation process followed a structured rubric focusing on:
              </p>
              <ul>
                <li>Coverage of the anomalous region</li>
                <li>Boundary accuracy</li>
                <li>Over-segmentation or under-segmentation</li>
                <li>Presence of noise or spurious artifacts</li>
              </ul>
              
              <h3 class="title is-5">Optimization Stage</h3>
              <p>
                The final stage, shown in the rightmost section of our pipeline diagram, leverages the preference data to enhance model performance. Starting with the SFT Policy (our supervised fine-tuned model), we apply Direct Preference Optimization (DPO) to align the model with human judgments. This process takes the ranked responses from the Annotation stage and optimizes the policy to favor segmentations that human annotators preferred, resulting in an Optimized Policy that better reflects human perceptual preferences.
              </p>
              <p>
                Unlike traditional reinforcement learning approaches that require complex reward modeling, DPO directly learns from pairwise preferences, making it particularly suitable for segmentation tasks where quality assessment is often subjective and multifaceted.
              </p>
              
              <div class="columns is-centered">
                <div class="column is-full">
                  <div class="columns">
                    <div class="column is-half">
                      <img src="static/images/sft_train.png" alt="SFT-SAM Training Loss" class="center-block" style="width: 100%; margin-bottom: 1rem;">
                      <p class="has-text-centered is-size-6">SFT-SAM training loss</p>
                    </div>
                    <div class="column is-half">
                      <img src="static/images/dpo_train.png" alt="VisionSAM Training Loss" class="center-block" style="width: 100%; margin-bottom: 1rem;">
                      <p class="has-text-centered is-size-6">VisionSAM (DPO) training loss</p>
                    </div>
                  </div>
                </div>
              </div>
              
              <p>
                This end-to-end pipeline represents a novel approach to fine-tuning vision foundation models, leveraging human feedback to improve specialized tasks like industrial anomaly segmentation without requiring extensive labeled datasets.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Methodology Section -->

    <!-- Experimental Results Section -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Experimental Results</h2>
            <div class="content has-text-justified">
              <h3 class="title is-5">Performance Comparison</h3>
              <p>
                Our experiments evaluated three model variants:
              </p>
              <ol>
                <li><strong>Original SAM</strong>: The unmodified ViT-B foundation model</li>
                <li><strong>SFT-SAM</strong>: SAM fine-tuned with supervised learning on our anomaly dataset</li>
                <li><strong>VisionSAM</strong>: Our RLHF-enhanced model using DPO</li>
              </ol>
              
              <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>IoU</th>
                    <th>Dice</th>
                    <th>Parameters</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>SAM</td>
                    <td>0.5818</td>
                    <td>0.7049</td>
                    <td>357.1 M</td>
                  </tr>
                  <tr>
                    <td>SFT-SAM</td>
                    <td>0.6584</td>
                    <td>0.7737</td>
                    <td>357.1 M</td>
                  </tr>
                  <tr>
                    <td>VisionSAM</td>
                    <td>0.6612 ↑</td>
                    <td>0.7790 ↑</td>
                    <td>375.1 M</td>
                  </tr>
                </tbody>
              </table>
              
              <p>
                Compared to the original SAM, our SFT-SAM achieved substantial performance gains with an IoU improvement of approximately +13.17% and a Dice improvement of +9.76%. This demonstrates the effectiveness of domain-specific supervised fine-tuning.
              </p>
              
              <p>
                After further fine-tuning via RLHF, VisionSAM achieved a modest improvement over SFT-SAM, with an additional +0.43% in IoU and +0.69% in Dice. While this gain indicates that preference-based learning can further refine the model, the overall impact was more subtle than expected.
              </p>
              
              <h3 class="title is-5">Key Findings</h3>
              
              <p>
                Interestingly, we discovered that applying DPO directly to the original SAM without prior supervised fine-tuning led to performance degradation. This RLHF-SAM variant showed a decrease of 2.27% in IoU and 1.67% in Dice compared to the original SAM.
              </p>
              
              <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>IoU</th>
                    <th>Dice</th>
                    <th>Parameters</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>SAM</td>
                    <td>0.5818</td>
                    <td>0.7049</td>
                    <td>357.1 M</td>
                  </tr>
                  <tr>
                    <td>RLHF-SAM (DPO only)</td>
                    <td>0.5686 ↓</td>
                    <td>0.6931 ↓</td>
                    <td>375.1 M</td>
                  </tr>
                </tbody>
              </table>
              
              <p>
                This finding suggests that preference-based fine-tuning requires a strong task-specific foundation to be effective. Without prior supervised grounding on domain-relevant demonstrations, DPO optimization may struggle to align preference gradients with representations that aren't yet sensitive to fine-grained anomaly features.
              </p>
              
              <p>
                Our results emphasize the importance of using supervised fine-tuning as a warm start before applying RLHF techniques in dense prediction tasks like segmentation, where alignment targets are less well-defined than in language generation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Experimental Results Section -->

  
    <!-- End Phase 2 Conclusions Section -->

    <!-- 
Youtube video
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      Paper video.
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            Youtube embed code here
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
End youtube video 
-->

<!-- Video carousel -->
    <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
          <h2 class="title">Poster to be added soon</h2>
        
          <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe> -->
      </div>
    </div>
  </section>
<!--End paper poster -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the
                <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
                project page. You are free to borrow the source code of this
                website, we just ask that you link back to this page in the
                footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
  </html>
