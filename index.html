<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"

   Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
  <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      Towards Smarter Segmentation: Improving SAM's Anomaly Understanding with
      RLHF
    </title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Towards Smarter Segmentation: Improving SAM's Anomaly
                Understanding with RLHF
              </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                  <a href="https://github.com/slmowan" target="_blank"
                    >Wan Wang</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://github.com/novia-aa" target="_blank"
                    >Yu-Tong Chuang</a
                  ><sup>*</sup>,
                </span>
                  <span class="author-block">
                  <a href="https://github.com/Charlesczc" target="_blank"
                    >Yiu Chang</a
                  ><sup>*</sup>,
                  </span>
                <span class="author-block">
                  <a href="https://github.com/lulinliu" target="_blank"
                    >Lulin Liu</a
                  ><sup>*</sup>,</span
                >
                  </div>

                  <div class="is-size-5 publication-authors">
                <span class="author-block"
                  >University of Minnesota, Twin Cities<br
                /></span>
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Indicates Equal Contribution</small
                  ></span
                >
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                    <a
                      href="static/pdfs/5541_course_project_final_report.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Report</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                    <a
                      href="static/pdfs/5541_poster_final_version.pptx.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/slmowan/sam-finetune.git"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

    <!-- 
Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
End teaser video 
-->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Industrial anomaly segmentation poses unique challenges due to the subtle, low-contrast nature of defects and the scarcity of high-quality annotations. While foundation models like Segment Anything Model (SAM) offer strong generalization in object-centric segmentation, they often fail to localize fine-grained anomalies. In this work, we make an initial attempt to adapt SAM to this setting via a two-stage pipeline: supervised fine-tuning (SFT) on a curated demonstration set, followed by preference-based finetuning using reinforcement learning from human feedback (RLHF).
          </p>
          <p>
            To facilitate this, we construct a small-scale comparison dataset with human-annotated segmentation preferences. Our method, VisionSAM, demonstrates modest yet consistent gains over the SFT baseline, achieving a +0.43% IoU and +0.69% Dice improvement on held-out categories. In contrast, directly applying RLHF to the original SAM results in performance degradation, underscoring the importance of task-specific grounding.
          </p>
          <p>
            Compared to existing fine-tuning strategies such as adapter-based or CLIP-integrated approaches, our method is the first to systematically introduce RLHF into dense prediction tasks. While results vary across prompt types and anomaly categories, our findings suggest that human-aligned feedback offers a promising supervision signal in low-data regimes, and opens a novel direction for human-in-the-loop vision foundation model adaptation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Datasets Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Datasets</h2>
        <div class="content has-text-justified">
          <p>
            We mainly utilize the <a href="https://huggingface.co/datasets/VISION-Workshop/VISION-Datasets" target="_blank">VISION dataset</a> for our project, which is a comprehensive and realistic benchmark specifically designed for industrial anomaly detection and segmentation. It includes 14 representative inspection categories (e.g., Cable, Capacitor), with approximately 18,000 high-resolution images captured across diverse manufacturing settings.
          </p>
          <p>
            The dataset's anomalies are often subtle, low-contrast, and embedded in complex visual contexts, making it a challenging and representative setting for evaluating the effectiveness of our proposed methods. We also used the <a href="https://www.mvtec.com/company/research/datasets/mvtec-ad" target="_blank">MVTec AD</a> dataset in our baseline analysis phase.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Datasets Section -->

<!-- Baseline Analysis Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Baseline Analysis: SAM's Response to Anomaly Prompts</h2>
        <div class="content has-text-justified">
          <p>
            Prior to fine-tuning, we evaluated SAM's zero-shot segmentation performance on industrial anomaly datasets to analyze its failure modes and performance boundaries. We uniformly sampled 210 test images from 14 distinct anomaly categories, with 10 to 15 representative instances per category, ensuring broad coverage across variations in defect shape, texture, and scale.
          </p>
          
          <p>
            We tested three prompt configurations: (1) Single positive point at the center of the defect, (2) Positive + Negative points to guide the model, and (3) Bounding box enclosing the target anomaly.
          </p>
          
          <h3 class="title is-5">Representative Results</h3>
          <p>Our analysis revealed four typical outcome patterns:</p>
          
          <div class="columns is-multiline">
            <div class="column is-full">
              <h4 class="title is-5">Case 1: Correct Segmentation, Aligned Confidence</h4>
              <div class="columns">
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Image</p>
                  <img src="figs/phase_1/metal_1.png" alt="Metal Image" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 1</p>
                  <img src="figs/phase_1/metal_2.png" alt="Metal Mask 1" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Ground Truth</p>
                  <img src="figs/phase_1/metal_3.png" alt="Metal Ground Truth" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 2</p>
                  <img src="figs/phase_1/metal_4.png" alt="Metal Mask 2" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 3</p>
                  <img src="figs/phase_1/metal_5.png" alt="Metal Mask 3" class="image">
                </div>
              </div>
              <p class="has-text-centered">
                In this example, SAM successfully produces a mask that closely matches the ground truth in both shape and location. Additionally, the mask with the highest confidence score corresponds to the best segmentation, indicating a strong alignment between model prediction and human judgment.
              </p>
            </div>
            
            <div class="column is-full mt-5">
              <h4 class="title is-5">Case 2: Correct Segmentation, Misaligned Confidence</h4>
              <div class="columns">
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Image</p>
                  <img src="figs/phase_1/crack_1.png" alt="Crack Image" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 1</p>
                  <img src="figs/phase_1/crack_2.png" alt="Crack Mask 1" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Ground Truth</p>
                  <img src="figs/phase_1/crack_3.png" alt="Crack Ground Truth" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 2</p>
                  <img src="figs/phase_1/crack_4.png" alt="Crack Mask 2" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 3</p>
                  <img src="figs/phase_1/crack_5.png" alt="Crack Mask 3" class="image">
                </div>
              </div>
              <p class="has-text-centered">
                In this case, SAM generates a reasonably accurate mask but assigns a higher confidence to a suboptimal one. This discrepancy suggests a misalignment between the model's internal scoring and human perception of quality, which might hinder downstream automation systems that rely on score-based mask selection.
              </p>
            </div>
            
            <div class="column is-full mt-5">
              <h4 class="title is-5">Case 3: Partial Failure due to Semantic Misunderstanding</h4>
              <div class="columns">
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Image</p>
                  <img src="figs/phase_1/missing_wire_1.png" alt="Wire Image" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 1</p>
                  <img src="figs/phase_1/missing_wire_2.png" alt="Wire Mask 1" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Ground Truth</p>
                  <img src="figs/phase_1/missing_wire_3.png" alt="Wire Ground Truth" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 2</p>
                  <img src="figs/phase_1/missing_wire_4.png" alt="Wire Mask 2" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 3</p>
                  <img src="figs/phase_1/missing_wire_5.png" alt="Wire Mask 3" class="image">
                </div>
              </div>
              <p class="has-text-centered">
                SAM sometimes segments entire object structures rather than focusing on the anomalous regions. Here, when tasked with segmenting a defect on a wire, SAM includes the inner cavity of the wire as part of the predicted mask. While the mask is spatially consistent, it reflects a lack of semantic understanding of what constitutes a defect.
              </p>
            </div>
            
            <div class="column is-full mt-5">
              <h4 class="title is-5">Case 4: Failure with Subtle Defects</h4>
              <div class="columns">
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Image</p>
                  <img src="figs/phase_1/carpet_1.png" alt="Carpet Image" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 1</p>
                  <img src="figs/phase_1/carpet_2.png" alt="Carpet Mask 1" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Ground Truth</p>
                  <img src="figs/phase_1/carpet_3.png" alt="Carpet Ground Truth" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 2</p>
                  <img src="figs/phase_1/carpet_4.png" alt="Carpet Mask 2" class="image">
                </div>
                <div class="column is-one-fifth">
                  <p class="has-text-centered is-size-7 mb-1">Mask 3</p>
                  <img src="figs/phase_1/carpet_5.png" alt="Carpet Mask 3" class="image">
                </div>
              </div>
              <p class="has-text-centered">
                In this example, SAM fails to produce any meaningful segmentation for a subtle anomaly in a carpet. The deformation is barely perceptible even to human observers, highlighting SAM's limitations when dealing with low-contrast defects in textured surfaces.
              </p>
            </div>
          </div>
          
          <p class="mt-5">
            Across all experiments, we observed that SAM often segments entire objects rather than focusing on local defects, suggesting it lacks an explicit notion of "anomaly." Point-based prompts, especially those combining positive and negative clicks, generally led to better results than bounding boxes, likely due to the finer spatial guidance they provide. These findings confirm our hypothesis that while SAM excels at general object segmentation, it struggles with industrial anomaly detection due to lacking domain-specific understanding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Baseline Analysis Section -->

    <!-- 
Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
End teaser video 
-->

<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Industrial anomaly segmentation poses unique challenges due to the subtle, low-contrast nature of defects and the scarcity of high-quality annotations. While foundation models like Segment Anything Model (SAM) offer strong generalization in object-centric segmentation, they often fail to localize fine-grained anomalies. In this work, we make an initial attempt to adapt SAM to this setting via a two-stage pipeline: supervised fine-tuning (SFT) on a curated demonstration set, followed by preference-based finetuning using reinforcement learning from human feedback (RLHF).
          </p>
          <p>
            To facilitate this, we construct a small-scale comparison dataset with human-annotated segmentation preferences. Our method, VisionSAM, demonstrates modest yet consistent gains over the SFT baseline, achieving a +0.43% IoU and +0.69% Dice improvement on held-out categories. In contrast, directly applying RLHF to the original SAM results in performance degradation, underscoring the importance of task-specific grounding.
          </p>
          <p>
            Compared to existing fine-tuning strategies such as adapter-based or CLIP-integrated approaches, our method is the first to systematically introduce RLHF into dense prediction tasks. While results vary across prompt types and anomaly categories, our findings suggest that human-aligned feedback offers a promising supervision signal in low-data regimes, and opens a novel direction for human-in-the-loop vision foundation model adaptation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->

<!-- Datasets Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Datasets</h2>
        <div class="content has-text-justified">
          <p>
            We mainly utilize the <a href="https://huggingface.co/datasets/VISION-Workshop/VISION-Datasets" target="_blank">VISION dataset</a> for our project, which is a comprehensive and realistic benchmark specifically designed for industrial anomaly detection and segmentation. It includes 14 representative inspection categories (e.g., Cable, Capacitor), with approximately 18,000 high-resolution images captured across diverse manufacturing settings.
          </p>
          <p>
            The dataset's anomalies are often subtle, low-contrast, and embedded in complex visual contexts, making it a challenging and representative setting for evaluating the effectiveness of our proposed methods. We also used the <a href="https://www.mvtec.com/company/research/datasets/mvtec-ad" target="_blank">MVTec AD</a> dataset in our baseline analysis phase.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Datasets Section -->

<!-- Project Progress Section -->
<!-- <section class="section"> -->
  <!-- <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Project Progress</h2>

        Phase 1
        <h3 class="title is-4">
          Phase 1: Benchmarking SAM's Anomaly Segmentation
        </h3>
        <div class="content has-text-justified">
          <p>
            We have successfully completed the first phase of our project,
            focusing on establishing SAM's baseline performance on anomaly
            segmentation tasks. Our evaluation revealed several key
            insights:
          </p>

          <ul>
            <li>
              <strong>Dataset Preparation:</strong> We processed multiple
              industrial anomaly detection datasets, standardizing formats
              and annotations to ensure consistent evaluation.
            </li>

            <li>
              <strong>Performance Analysis:</strong> While SAM excels at
              general segmentation tasks, we identified systematic
              limitations when handling industrial anomalies, particularly
              subtle surface defects and texture irregularities.
            </li>

            <li>
              <strong>Failure Case Categorization:</strong> We categorized
              SAM's failure patterns into three main types: missed
              detections (false negatives), imprecise boundary delineation,
              and over-segmentation of normal regions.
            </li>

            <li>
              <strong>Confidence Analysis:</strong> We measured SAM's
              prediction confidence across different types of anomalies,
              revealing that SAM often exhibits high confidence even for
              incorrect segmentations, highlighting the need for
              domain-specific adaptation.
            </li>
          </ul>

          <p>
            These findings establish a critical baseline for measuring
            improvement in subsequent phases and confirm our hypothesis that
            while powerful, foundation models like SAM require specialized
            fine-tuning to excel at domain-specific tasks like industrial
            anomaly detection.
          </p>

          <p>
            Moving forward, we are now working on Phase 2, developing a
            domain-specific baseline model inspired by MedSAM's approach to
            medical image segmentation, adapting these techniques to the
            industrial anomaly context.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End Project Progress Section -->

<!-- Image carousel section with summaries -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
              <img
                src="static/images/carousel1.jpg"
                alt="SAM Anomaly Segmentation - Bottle"
              />
        <h2 class="subtitle has-text-centered">
                <strong>Good Segmentation, High Confidence - Bottle</strong
                ><br />
                SAM correctly identifies and segments the anomaly with high
                confidence, showing its potential when anomalies have clear
                boundaries and distinct features from the background.
        </h2>
      </div>
      <div class="item">
              <img
                src="static/images/carousel2.jpg"
                alt="SAM Anomaly Segmentation - Capsule"
              />
        <h2 class="subtitle has-text-centered">
                <strong>Poor Segmentation, High Confidence - Carpet</strong
                ><br />
                SAM incorrectly segments the anomaly while expressing high
                confidence, demonstrating how misleading confidence scores can
                be for textured surfaces where anomalies have subtle differences
                from the background pattern.
        </h2>
      </div>
      <div class="item">
              <img
                src="static/images/carousel3.jpg"
                alt="SAM Anomaly Segmentation - Hazelnut"
              />
        <h2 class="subtitle has-text-centered">
                <strong>Good Segmentation, Low Confidence - Hazelnut</strong
                ><br />
                SAM accurately detects the anomaly but with low confidence,
                showing how even correct predictions can be uncertain,
                suggesting potential for performance improvement through
                confidence calibration.
       </h2>
     </div>
     <div class="item">
              <img
                src="static/images/carousel4.jpg"
                alt="SAM Anomaly Segmentation - Plate"
              />
      <h2 class="subtitle has-text-centered">
                <strong>Poor Segmentation, Low Confidence - Plate</strong><br />
                SAM fails to detect the anomaly and expresses low confidence,
                highlighting the most challenging case where both segmentation
                and confidence need improvement through specialized training.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->

<!-- Phase 2 Section -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">
          Phase 2: Domain-Specific Model Development
        </h3>
        <div class="content has-text-justified">
          <p>
            We have made significant progress in Phase 2, focusing on data
            preprocessing and adapting SAM for industrial anomaly detection.
            Our work has successfully addressed several technical
            challenges:
          </p>

          <h4 class="title is-5">Data Preprocessing Innovations</h4>
          <ul>
            <li>
              <strong>Input Resolution Standardization:</strong> SAM
              requires 1024×1024 resolution inputs, but industrial images
              come in various dimensions. We implemented a robust
              preprocessing pipeline that preserves aspect ratios while
              meeting SAM's requirements:
            </li>
            <ul>
              <li>
                For non-square images, we apply careful padding to both
                images and their corresponding ground truth masks, ensuring
                spatial relationships are maintained
              </li>
              <li>
                This approach preserves the relative size and location of
                anomalies, which is critical for accurate segmentation
              </li>
            </ul>

            <li>
              <strong>Multiple Anomaly Handling:</strong> Industrial images
              often contain multiple anomalies with separate bounding boxes.
              We developed a sophisticated approach to handle these cases:
            </li>
            <ul>
              <li>
                For images with multiple anomalies, we create individual
                copies of the image-mask pairs for each anomaly
              </li>
              <li>
                This transformation allows the model to focus on one anomaly
                at a time, simplifying the learning task while preserving
                the original image context
              </li>
              <li>
                During inference, predictions from multiple instances are
                intelligently merged to provide a complete anomaly map
              </li>
            </ul>
          </ul>

          <h4 class="title is-5">Dataset Preparation</h4>
          <p>
            Our preprocessing pipeline has been successfully applied to
            multiple industrial anomaly datasets, including MVTec AD and
            other domain-specific collections. This standardization enables:
          </p>
          <ul>
            <li>
              Consistent training across diverse industrial products and
              defect types
            </li>
            <li>
              Effective utilization of SAM's existing capabilities while
              adapting to industrial anomaly detection
            </li>
            <li>
              Creation of a robust foundation for the upcoming RLHF training
              phase
            </li>
          </ul>

          <p>
            As we complete Phase 2, we are preparing to initiate the RLHF
            pipeline development. The standardized dataset will serve as the
            foundation for generating multiple segmentation candidates per
            image, which will then be ranked by human annotators to train
            our reward model.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End Phase 2 Section -->

<!-- Future Work Section -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Future Work</h2>
        <div class="content has-text-justified">
          <p>
            Building on our current progress, we have outlined a comprehensive plan for the next phases of our project:
          </p>

          <h3 class="title is-5">Phase 3: RLHF-Based Training Pipeline Development</h3>
          <p>
            Our next major phase focuses on implementing the RLHF pipeline through human feedback collection and reward model training. Key deliverables include:
          </p>
          <ul>
            <li><strong>Data Pipeline Development:</strong>
              <ul>
                <li>Implement confidence-based mask generation system</li>
                <li>Create automated pipeline for generating segmentation variants</li>
                <li>Develop data processing workflows for human feedback</li>
              </ul>
            </li>
            
            <li><strong>Human Annotation Platform:</strong>
              <ul>
                <li>Design and implement interface for collecting segmentation rankings</li>
                <li>Create efficient workflow for annotators to provide feedback</li>
                <li>Implement quality control measures for feedback collection</li>
              </ul>
            </li>
            
            <li><strong>Reward Model Development:</strong>
              <ul>
                <li>Train initial reward model using collected human preferences</li>
                <li>Implement evaluation metrics for segmentation quality</li>
                <li>Develop feedback integration mechanisms</li>
              </ul>
            </li>
          </ul>

          <h3 class="title is-5">Phase 4: Fine-tuning with DPO</h3>
          <p>
            The final phase will focus on implementing Direct Preference Optimization to enhance SAM's anomaly segmentation capabilities:
          </p>
          <ul>
            <li><strong>Model Enhancement:</strong>
              <ul>
                <li>Implement DPO-based training pipeline</li>
                <li>Optimize model parameters using human preference data</li>
                <li>Fine-tune segmentation performance for industrial anomalies</li>
              </ul>
            </li>
            
            <li><strong>Performance Analysis:</strong>
              <ul>
                <li>Conduct comparative analysis between baseline and DPO-enhanced models</li>
                <li>Evaluate improvements in segmentation accuracy and confidence calibration</li>
                <li>Assess generalization to different types of industrial anomalies</li>
              </ul>
            </li>
            
            <li><strong>Documentation and Reporting:</strong>
              <ul>
                <li>Prepare comprehensive documentation of methodology and results</li>
                <li>Generate final research report with key insights</li>
                <li>Create deployment guidelines for industrial applications</li>
              </ul>
            </li>
          </ul>

          <p>
            Through these phases, we aim to demonstrate the effectiveness of DPO in improving SAM's anomaly detection capabilities while maintaining training stability and efficiency. Our focus will be on creating a practical, deployable solution for industrial applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End Future Work Section -->

<!-- Methodology Section - Updated with new pipeline image -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            We've implemented a three-stage RLHF-based segmentation fine-tuning workflow to improve SAM's anomaly detection capabilities:
          </p>
          
          <div class="columns is-centered">
            <div class="column is-full">
              <img src="static/images/pipeline.png" alt="Three-stage RLHF Pipeline" class="center-block" style="width: 100%; margin-bottom: 1.5rem;">
              <p class="has-text-centered is-size-6">Overview of our three-stage RLHF workflow for segmentation fine-tuning</p>
            </div>
          </div>
          
          <p>
            As shown in our pipeline diagram, our approach consists of three key stages that work together to leverage human feedback for improving model performance:
          </p>
          
          <h3 class="title is-5">Generation Stage</h3>
          <p>
            In the first stage, we combine SFT (Supervised Fine-Tuning) images with prompts to generate segmentation candidates. A fine-tuned SAM model processes these inputs through the SAM Mask Generation component, producing multiple segmentation masks for each prompt (shown as "Per-prompt Responses" in the diagram). This stage establishes a foundation of domain-specific knowledge through supervised learning.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half">
              <img src="static/images/sam_triplet_sample.png" alt="SAM Triplet Sample" class="center-block" style="width: 100%; margin-bottom: 1.5rem;">
              <p class="has-text-centered is-size-6">Example of a SAM-compatible triplet training sample</p>
            </div>
          </div>
          
          <h3 class="title is-5">Annotation Stage</h3>
          <p>
            The second stage introduces human judgment into the pipeline. As illustrated in the middle section of our diagram, the SFT images and corresponding mask candidates from the Generation stage are presented to human annotators who rank the segmentation options according to quality. This Human Ranking process transforms unordered segmentation candidates into preference-ranked responses, creating valuable training signal for the next stage.
          </p>
          <p>
            Our annotation process followed a structured rubric focusing on:
          </p>
          <ul>
            <li>Coverage of the anomalous region</li>
            <li>Boundary accuracy</li>
            <li>Over-segmentation or under-segmentation</li>
            <li>Presence of noise or spurious artifacts</li>
          </ul>
          
          <h3 class="title is-5">Optimization Stage</h3>
          <p>
            The final stage, shown in the rightmost section of our pipeline diagram, leverages the preference data to enhance model performance. Starting with the SFT Policy (our supervised fine-tuned model), we apply Direct Preference Optimization (DPO) to align the model with human judgments. This process takes the ranked responses from the Annotation stage and optimizes the policy to favor segmentations that human annotators preferred, resulting in an Optimized Policy that better reflects human perceptual preferences.
          </p>
          <p>
            Unlike traditional reinforcement learning approaches that require complex reward modeling, DPO directly learns from pairwise preferences, making it particularly suitable for segmentation tasks where quality assessment is often subjective and multifaceted.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-full">
              <div class="columns">
                <div class="column is-half">
                  <img src="static/images/sft_train.png" alt="SFT-SAM Training Loss" class="center-block" style="width: 100%; margin-bottom: 1rem;">
                  <p class="has-text-centered is-size-6">SFT-SAM training loss</p>
                </div>
                <div class="column is-half">
                  <img src="static/images/dpo_train.png" alt="VisionSAM Training Loss" class="center-block" style="width: 100%; margin-bottom: 1rem;">
                  <p class="has-text-centered is-size-6">VisionSAM (DPO) training loss</p>
                </div>
              </div>
            </div>
          </div>
          
          <p>
            This end-to-end pipeline represents a novel approach to fine-tuning vision foundation models, leveraging human feedback to improve specialized tasks like industrial anomaly segmentation without requiring extensive labeled datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methodology Section -->

<!-- Experimental Results Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <h3 class="title is-5">Performance Comparison</h3>
          <p>
            Our experiments evaluated three model variants:
          </p>
          <ol>
            <li><strong>Original SAM</strong>: The unmodified ViT-B foundation model</li>
            <li><strong>SFT-SAM</strong>: SAM fine-tuned with supervised learning on our anomaly dataset</li>
            <li><strong>VisionSAM</strong>: Our RLHF-enhanced model using DPO</li>
          </ol>
          
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Model</th>
                <th>IoU</th>
                <th>Dice</th>
                <th>Parameters</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SAM</td>
                <td>0.5818</td>
                <td>0.7049</td>
                <td>357.1 M</td>
              </tr>
              <tr>
                <td>SFT-SAM</td>
                <td>0.6584</td>
                <td>0.7737</td>
                <td>357.1 M</td>
              </tr>
              <tr>
                <td>VisionSAM</td>
                <td>0.6612 ↑</td>
                <td>0.7790 ↑</td>
                <td>375.1 M</td>
              </tr>
            </tbody>
          </table>
          
          <p>
            Compared to the original SAM, our SFT-SAM achieved substantial performance gains with an IoU improvement of approximately +13.17% and a Dice improvement of +9.76%. This demonstrates the effectiveness of domain-specific supervised fine-tuning.
          </p>
          
          <p>
            After further fine-tuning via RLHF, VisionSAM achieved a modest improvement over SFT-SAM, with an additional +0.43% in IoU and +0.69% in Dice. While this gain indicates that preference-based learning can further refine the model, the overall impact was more subtle than expected.
          </p>
          
          <h3 class="title is-5">Key Findings</h3>
          
          <p>
            Interestingly, we discovered that applying DPO directly to the original SAM without prior supervised fine-tuning led to performance degradation. This RLHF-SAM variant showed a decrease of 2.27% in IoU and 1.67% in Dice compared to the original SAM.
          </p>
          
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Model</th>
                <th>IoU</th>
                <th>Dice</th>
                <th>Parameters</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SAM</td>
                <td>0.5818</td>
                <td>0.7049</td>
                <td>357.1 M</td>
              </tr>
              <tr>
                <td>RLHF-SAM (DPO only)</td>
                <td>0.5686 ↓</td>
                <td>0.6931 ↓</td>
                <td>375.1 M</td>
              </tr>
            </tbody>
          </table>
          
          <p>
            This finding suggests that preference-based fine-tuning requires a strong task-specific foundation to be effective. Without prior supervised grounding on domain-relevant demonstrations, DPO optimization may struggle to align preference gradients with representations that aren't yet sensitive to fine-grained anomaly features.
          </p>
          
          <p>
            Our results emphasize the importance of using supervised fine-tuning as a warm start before applying RLHF techniques in dense prediction tasks like segmentation, where alignment targets are less well-defined than in language generation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experimental Results Section -->

  
    <!-- End Phase 2 Conclusions Section -->

    <!-- 
Youtube video
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      Paper video.
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            Youtube embed code here
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
End youtube video 
-->

<!-- Video carousel -->
    <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
          <h2 class="title">Poster to be added soon</h2>
        
          <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe> -->
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->

  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the
                <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
                project page. You are free to borrow the source code of this
                website, we just ask that you link back to this page in the
                footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

<!-- Visual Comparison Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visual Comparisons</h2>
        <div class="content has-text-justified">
          <p>
            Visual comparisons between our models highlight both the improvements achieved through our proposed methods and the remaining challenges.
          </p>
          
          <div class="columns is-multiline">
            <div class="column is-full">
              <figure class="image">
                <img src="static/images/wood_034.png" alt="Comparison between the original SAM and SFT-SAM">
                <figcaption class="has-text-centered mt-2">Comparison between the original SAM and SFT-SAM on a wood anomaly sample. SFT-SAM correctly identifies the defect area while SAM fails to differentiate it from the background texture.</figcaption>
              </figure>
            </div>
            
            <div class="column is-full mt-5">
              <figure class="image">
                <img src="static/images/wood_026_noise.png" alt="Comparison between SFT-SAM and VisionSAM">
                <figcaption class="has-text-centered mt-2">Comparison between SFT-SAM and VisionSAM on another wood sample. While VisionSAM attempts to capture a subtle scratch, it produces scattered noise rather than a coherent segmentation, highlighting the challenges of further refinement.</figcaption>
              </figure>
            </div>
            
            <!-- <div class="column is-full mt-5">
              <figure class="image">
                <img src="static/images/screw_039.png" alt="Example of successful segmentation">
                <figcaption class="has-text-centered mt-2">A successful case where our approach correctly segments a defect in a screw sample, demonstrating the effectiveness of domain-specific fine-tuning for certain anomaly types.</figcaption>
              </figure>
            </div> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Visual Comparison Section -->

<!-- Limitations and Future Work Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Limitations and Future Work</h2>
        <div class="content has-text-justified">
          <h3 class="title is-5">Limitations</h3>
          <p>
            Despite the promising results, our approach has several limitations:
          </p>
          <ul>
            <li>
              <strong>Modest RLHF Gains:</strong> The overall performance improvement from RLHF fine-tuning is modest, with only incremental gains over the supervised fine-tuned baseline. This suggests that preference signals alone may be insufficient to significantly shift the behavior of a model already aligned with the task via supervision.
            </li>
            <li>
              <strong>Challenging Defect Types:</strong> VisionSAM occasionally struggles with capturing fine-grained structures and may hallucinate small disconnected components, particularly on challenging categories with low contrast or irregular defect patterns.
            </li>
            <li>
              <strong>Computational Overhead:</strong> While our method remains efficient in terms of model size, generating and comparing multiple masks during inference incurs additional computational overhead that could impact real-time applications.
            </li>
          </ul>
          
          <h3 class="title is-5">Future Work</h3>
          <p>
            Several promising directions for future research include:
          </p>
          <ul>
            <li>
              <strong>Better Anomaly Injection:</strong> Exploring methods to more effectively inject the concept of "anomaly" into the model, potentially through pretraining on anomaly-centric datasets or designing prompts that explicitly highlight abnormal regions.
            </li>
            <li>
              <strong>Richer Feedback Forms:</strong> Moving beyond binary comparisons to incorporate more nuanced forms of human feedback, such as ranked masks, region-level critiques, or confidence calibration from domain experts.
            </li>
            <li>
              <strong>Expanded Dataset:</strong> Building a larger preference dataset with more diverse annotators to improve generalization and reduce potential biases in the preference learning process.
            </li>
            <li>
              <strong>Alternative Architectures:</strong> Investigating different parameter-efficient fine-tuning approaches, such as LoRA or adapters, to reduce computational requirements while maintaining or improving performance.
            </li>
          </ul>
          
          <p>
            These directions could further enhance the practical applicability of vision foundation models for industrial anomaly detection and other specialized segmentation tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Limitations and Future Work Section -->
  </body>
  </html>
